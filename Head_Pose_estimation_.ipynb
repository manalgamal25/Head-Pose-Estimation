{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UBOpofo0YWy"
      },
      "source": [
        "## Head Pose Estimation Project\n",
        "\n",
        "This project uses MediaPipe Face Mesh and Support Vector Regression (SVR) to estimate head pose (pitch, yaw, roll) from facial landmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9lTcJ1BVjL9"
      },
      "outputs": [],
      "source": [
        "# Install required packages using: pip install -r requirements.txt\n",
        "# Or install manually: pip install mediapipe opencv-python numpy scipy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1eh6avc1QEo"
      },
      "source": [
        "##Import libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk1MpXCPVowt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import glob\n",
        "import os\n",
        "import scipy.io as sio\n",
        "from math import cos, sin\n",
        "import mediapipe\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Libraries for model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = 'AFLW2000'  # Directory containing the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6cNKf7c0SWd"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "**Note:** Download the AFLW2000-3D dataset from:\n",
        "http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/Database/AFLW2000-3D.zip\n",
        "\n",
        "Extract it to the project directory. The dataset should be in the `AFLW2000` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeGMaMt8VwDO"
      },
      "outputs": [],
      "source": [
        "# Download the dataset manually or use the following code:\n",
        "# import urllib.request\n",
        "# url = 'http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/Database/AFLW2000-3D.zip'\n",
        "# urllib.request.urlretrieve(url, 'AFLW2000-3D.zip')\n",
        "# \n",
        "# Then extract: import zipfile\n",
        "# with zipfile.ZipFile('AFLW2000-3D.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtkoG7XVV-eY"
      },
      "outputs": [],
      "source": [
        "# Check if data directory exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"Warning: {DATA_DIR} directory not found. Please download and extract the dataset.\")\n",
        "else:\n",
        "    print(f\"Data directory found: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUgTduAo1a2b"
      },
      "source": [
        "##Read the images & extract the features from the face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nvfrCRRWVt_"
      },
      "outputs": [],
      "source": [
        "images_dir = glob.glob(f'{DATA_DIR}/*.jpg')\n",
        "print(f\"Found {len(images_dir)} images\")\n",
        "\n",
        "# Empty list to store the image arrays\n",
        "features_data = []\n",
        "labels_data = []\n",
        "faceModule = mediapipe.solutions.face_mesh\n",
        "\n",
        "with faceModule.FaceMesh(static_image_mode=True) as faces:\n",
        "    # Loop over all the images in the directory\n",
        "    for img_path in images_dir:\n",
        "        # Read the image and corresponding label \n",
        "        base_name = os.path.splitext(img_path)[0]\n",
        "        image = cv2.imread(base_name + '.jpg')\n",
        "        \n",
        "        if image is None:\n",
        "            print(f\"Warning: Could not read image {base_name}.jpg\")\n",
        "            continue\n",
        "            \n",
        "        label_path = base_name + '.mat'\n",
        "        if not os.path.exists(label_path):\n",
        "            print(f\"Warning: Label file not found: {label_path}\")\n",
        "            continue\n",
        "            \n",
        "        label = sio.loadmat(label_path)['Pose_Para'][0][:3]\n",
        "        \n",
        "        # Processing the face to extract the landmark points (468 points) for each x,y\n",
        "        results = faces.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        if results.multi_face_landmarks is not None: \n",
        "            # Looping over the faces in the image\n",
        "            for face in results.multi_face_landmarks:\n",
        "                x_points = []\n",
        "                y_points = []\n",
        "                for landmark in face.landmark:\n",
        "                    x = landmark.x\n",
        "                    y = landmark.y\n",
        "                    x_points.append(x)\n",
        "                    y_points.append(y)\n",
        "                x_point = np.array(x_points)\n",
        "                y_point = np.array(y_points)\n",
        "                x_center = x_point - x_point[0]\n",
        "                y_center = y_point - y_point[0]   \n",
        "                features_data.append(np.hstack([x_center, y_center]))\n",
        "                labels_data.append(label)\n",
        "\n",
        "# Convert the lists to NumPy arrays\n",
        "features = np.array(features_data)\n",
        "labels = np.array(labels_data)\n",
        "\n",
        "print(f\"Extracted features from {len(features)} faces\")\n",
        "print(f\"Features shape: {features.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zhntGjpNgNa2"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "y_train_pitch= y_train[:,0]\n",
        "y_train_yaw= y_train[:,1]\n",
        "y_train_roll= y_train[:,2]\n",
        "\n",
        "y_test_pitch= y_test[:,0]\n",
        "y_test_yaw= y_test[:,1]\n",
        "y_test_roll= y_test[:,2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mgjjETHXWLam"
      },
      "outputs": [],
      "source": [
        "# Create a SVR model\n",
        "pitch_model = SVR()\n",
        "yaw_model = SVR()\n",
        "roll_model = SVR()\n",
        "\n",
        "# Train the model on the training set\n",
        "pitch_model.fit(X_train, y_train_pitch)\n",
        "yaw_model.fit(X_train, y_train_yaw)\n",
        "roll_model.fit(X_train, y_train_roll)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_pitch = pitch_model.predict(X_test)\n",
        "y_pred_yaw = yaw_model.predict(X_test)\n",
        "y_pred_roll = roll_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1eOCkbEgg38P"
      },
      "outputs": [],
      "source": [
        "score = cross_validate(pitch_model,features,labels[:,0],cv=5,scoring=[\"neg_mean_absolute_error\",'r2'])\n",
        "\n",
        "score = cross_validate(yaw_model,features,labels[:,1],cv=5,scoring=[\"neg_mean_absolute_error\",'r2'])\n",
        "\n",
        "score = cross_validate(roll_model,features,labels[:,2],cv=5,scoring=[\"neg_mean_absolute_error\",'r2'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "smSf8Y5LFGba"
      },
      "outputs": [],
      "source": [
        "#function to draw the pitch,yaw and roll \n",
        "def draw_axis(img, pitch,yaw,roll, tdx=None, tdy=None, size = 100):\n",
        "\n",
        "    yaw = -yaw\n",
        "    if tdx != None and tdy != None:\n",
        "        tdx = tdx\n",
        "        tdy = tdy\n",
        "    else:\n",
        "        height, width = img.shape[:2]\n",
        "        tdx = width / 2\n",
        "        tdy = height / 2\n",
        "\n",
        "    # X-Axis pointing to right. drawn in red\n",
        "    x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
        "    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
        "\n",
        "    # Y-Axis | drawn in green\n",
        "    #        v\n",
        "    x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
        "    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
        "\n",
        "    # Z-Axis (out of the screen) drawn in blue\n",
        "    x3 = size * (sin(yaw)) + tdx\n",
        "    y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
        "\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),3)\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),3)\n",
        "    cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),2)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-OevAAJF5yA"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "# frames = ['image1.jpg', 'image2.jpg', 'image3.jpg']\n",
        "# processed_images = draw_images(frames)\n",
        "\n",
        "def draw_images(frames):\n",
        "  features_data_test = []\n",
        "  images = []\n",
        "  faceModule = mediapipe.solutions.face_mesh\n",
        "  with faceModule.FaceMesh(static_image_mode=True) as faces:\n",
        "    for img in frames:\n",
        "      try:\n",
        "         image = cv2.imread(img)\n",
        "      except:\n",
        "        image = img\n",
        "      # processing the face to extract the landmark points (468 point) for each x,y\n",
        "      results = faces.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      if results.multi_face_landmarks != None: \n",
        "        # looping over the faces in the image\n",
        "        for face in results.multi_face_landmarks:\n",
        "            x_points = []\n",
        "            y_points = []\n",
        "            for landmark in face.landmark:\n",
        "                x = landmark.x\n",
        "                y = landmark.y\n",
        "                # note: the x and y values are scaled to the their width and height so we will get back their actual value in the image\n",
        "                shape = image.shape \n",
        "                x_points.append(x)\n",
        "                y_points.append(y)\n",
        "            x_point = np.array(x_points)\n",
        "            y_point = np.array(y_points)\n",
        "            x_center = x_point - x_point[0]\n",
        "            y_center = y_point - y_point[0]   \n",
        "            features =np.hstack([x_center,y_center]).reshape(1,-1)\n",
        "            # Convert the list to a NumPy array\n",
        "            y_pred_pitch = pitch_model.predict(features)\n",
        "            y_pred_yaw = yaw_model.predict(features)\n",
        "            y_pred_roll = roll_model.predict(features)\n",
        "            draw_axis(image,y_pred_pitch,y_pred_yaw,y_pred_roll,x_points[1]*shape[1],y_points[1]*shape[0])\n",
        "            images.append(image)\n",
        "  return images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OiTHgdp21FR"
      },
      "source": [
        "## Testing the model on the image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TBG9iDajGnMn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7475SMHD3AmQ"
      },
      "source": [
        "## Testing the model on a video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLjIxMjeJGAn"
      },
      "outputs": [],
      "source": [
        "# Configuration for video processing\n",
        "INPUT_VIDEO = 'input_video.mp4'  # Change this to your input video path\n",
        "OUTPUT_VIDEO = 'output.mp4'      # Output video path\n",
        "\n",
        "if not os.path.exists(INPUT_VIDEO):\n",
        "    print(f\"Error: Input video not found: {INPUT_VIDEO}\")\n",
        "    print(\"Please update INPUT_VIDEO variable with the correct path.\")\n",
        "else:\n",
        "    cap = cv2.VideoCapture(INPUT_VIDEO)\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, frame_rate, (width, height))\n",
        "    \n",
        "    frames = []\n",
        "    while True:\n",
        "        try:\n",
        "            # Read the next frame from the video\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.flip(frame, 1)\n",
        "            frames.append(frame)\n",
        "        except Exception as error:\n",
        "            print(error)\n",
        "            break\n",
        "    \n",
        "    print(f\"Processing {len(frames)} frames...\")\n",
        "    processed_frames = draw_images(frames)\n",
        "    \n",
        "    for frame in processed_frames:\n",
        "        out.write(frame)\n",
        "    \n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Output video saved to: {OUTPUT_VIDEO}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
